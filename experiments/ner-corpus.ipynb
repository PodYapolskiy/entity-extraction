{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Named Entity Recognition(NER)"]},{"cell_type":"markdown","metadata":{},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:06:46.837164Z","iopub.status.busy":"2024-10-06T13:06:46.836541Z","iopub.status.idle":"2024-10-06T13:06:46.841632Z","shell.execute_reply":"2024-10-06T13:06:46.840714Z","shell.execute_reply.started":"2024-10-06T13:06:46.837130Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import ast"]},{"cell_type":"markdown","metadata":{},"source":["# Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:06:46.843613Z","iopub.status.busy":"2024-10-06T13:06:46.843296Z","iopub.status.idle":"2024-10-06T13:06:46.853999Z","shell.execute_reply":"2024-10-06T13:06:46.853264Z","shell.execute_reply.started":"2024-10-06T13:06:46.843577Z"},"trusted":true},"outputs":[],"source":["def loading_data(data_path):\n","    \n","    data = pd.read_csv(data_path)\n","    \n","    data.dropna(inplace=True)\n","    print(\"Number of rows : \",data.shape[0],\" and the number of columns : \",data.shape[1])\n","    \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:06:46.856547Z","iopub.status.busy":"2024-10-06T13:06:46.855863Z","iopub.status.idle":"2024-10-06T13:06:47.161554Z","shell.execute_reply":"2024-10-06T13:06:47.160642Z","shell.execute_reply.started":"2024-10-06T13:06:46.856512Z"},"trusted":true},"outputs":[],"source":["data = loading_data(\"/kaggle/input/named-entity-recognition-ner-corpus/ner.csv\")\n","\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:06:47.163215Z","iopub.status.busy":"2024-10-06T13:06:47.162822Z","iopub.status.idle":"2024-10-06T13:06:47.169770Z","shell.execute_reply":"2024-10-06T13:06:47.168792Z","shell.execute_reply.started":"2024-10-06T13:06:47.163180Z"},"trusted":true},"outputs":[],"source":["data['POS'][0]"]},{"cell_type":"markdown","metadata":{},"source":["# Data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:06:47.172903Z","iopub.status.busy":"2024-10-06T13:06:47.171997Z","iopub.status.idle":"2024-10-06T13:06:47.178738Z","shell.execute_reply":"2024-10-06T13:06:47.177961Z","shell.execute_reply.started":"2024-10-06T13:06:47.172856Z"},"trusted":true},"outputs":[],"source":["def preprocess_data(data):\n","    for i in range(len(data)):\n","        pos = ast.literal_eval(data['POS'][i])\n","        tags = ast.literal_eval(data['Tag'][i])\n","        data['POS'][i] = [str(word) for word in pos]\n","        data['Tag'][i] = [str(word.upper()) for word in tags]\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:06:47.179993Z","iopub.status.busy":"2024-10-06T13:06:47.179747Z","iopub.status.idle":"2024-10-06T13:07:32.785407Z","shell.execute_reply":"2024-10-06T13:07:32.784345Z","shell.execute_reply.started":"2024-10-06T13:06:47.179971Z"},"trusted":true},"outputs":[],"source":["data = preprocess_data(data)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:07:32.787248Z","iopub.status.busy":"2024-10-06T13:07:32.786882Z","iopub.status.idle":"2024-10-06T13:07:32.793584Z","shell.execute_reply":"2024-10-06T13:07:32.792602Z","shell.execute_reply.started":"2024-10-06T13:07:32.787220Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def lower_text(text: str):\n","    return text.lower()\n","\n","def remove_punctuation(text: str):\n","    \"\"\"\n","    Substitute all punctiations with space in case of\n","    \"hello!nice to meet you\"\n","    \n","    If subs with '' -> \"hellonice to meet you\"\n","    With ' ' -> \"hello nice to meet you\"\n","    \"\"\"\n","    text_nopunct = re.sub('[^A-Za-z0-9\\s]', '', text)\n","    return text_nopunct\n","\n","def remove_multiple_spaces(text: str):\n","    text_no_doublespace = re.sub('\\s+', ' ', text)\n","    return text_no_doublespace"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:07:32.796003Z","iopub.status.busy":"2024-10-06T13:07:32.794950Z","iopub.status.idle":"2024-10-06T13:07:32.807445Z","shell.execute_reply":"2024-10-06T13:07:32.806352Z","shell.execute_reply.started":"2024-10-06T13:07:32.795975Z"},"trusted":true},"outputs":[],"source":["sample_text = data['Sentence'][3]\n","\n","_lowered = lower_text(sample_text)\n","_without_punct = remove_punctuation(_lowered)\n","_single_spaced = remove_multiple_spaces(_without_punct)\n","\n","print(sample_text)\n","print('-'*10)\n","print(_lowered)\n","print('-'*10)\n","print(_without_punct)\n","print('-'*10)\n","print(_single_spaced)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:07:32.809198Z","iopub.status.busy":"2024-10-06T13:07:32.808790Z","iopub.status.idle":"2024-10-06T13:07:32.819515Z","shell.execute_reply":"2024-10-06T13:07:32.818648Z","shell.execute_reply.started":"2024-10-06T13:07:32.809147Z"},"trusted":true},"outputs":[],"source":["from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import *\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","\n","stopWords = set(stopwords.words('english'))\n","\n","def tokenize_text(text: str) -> list[str]:\n","    return word_tokenize(text)\n","\n","def remove_stop_words(tokenized_text: list[str]) -> list[str]:\n","    wordsFiltered = [w for w in tokenized_text if w not in stopWords]\n","    return wordsFiltered\n","\n","def stem_words(tokenized_text: list[str]) -> list[str]:\n","    stemmer = WordNetLemmatizer()\n","    output = [stemmer.lemmatize(text) for text in tokenized_text]\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:07:32.820997Z","iopub.status.busy":"2024-10-06T13:07:32.820630Z","iopub.status.idle":"2024-10-06T13:07:32.830496Z","shell.execute_reply":"2024-10-06T13:07:32.829586Z","shell.execute_reply.started":"2024-10-06T13:07:32.820969Z"},"trusted":true},"outputs":[],"source":["def preprocessing_stage(text):\n","    _lowered = lower_text(text)\n","    _without_punct = remove_punctuation(_lowered)\n","    _single_spaced = remove_multiple_spaces(_without_punct)\n","    _tokenized = tokenize_text(_single_spaced)\n","#     _without_sw = remove_stop_words(_tokenized)\n","    _stemmed = stem_words(_tokenized)\n","    _stemmed = ' '.join(_stemmed)\n","    \n","    return _stemmed\n","\n","def clean_text_inplace(df):\n","    df['Sentence'] = df['Sentence'].apply(preprocessing_stage)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T13:07:32.835015Z","iopub.status.busy":"2024-10-06T13:07:32.834735Z"},"trusted":true},"outputs":[],"source":["nltk.download(\"wordnet\")\n","nltk.download(\"omw-1.4\")\n","!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","\n","\n","data = clean_text_inplace(data)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_final = data[['Sentence','Tag']]\n","\n","df_train, df_test = train_test_split(df_final,test_size=0.2,random_state=42)\n","len(df_train), len(df_test)"]},{"cell_type":"markdown","metadata":{},"source":["# Import model libraries and Make RNN model"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T14:48:38.153779Z","iopub.status.busy":"2024-10-06T14:48:38.153033Z","iopub.status.idle":"2024-10-06T14:48:38.163869Z","shell.execute_reply":"2024-10-06T14:48:38.162890Z","shell.execute_reply.started":"2024-10-06T14:48:38.153740Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from keras.losses import SparseCategoricalCrossentropy\n","from keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_targets = list(df_train.Tag.values)\n","test_targets = list(df_test.Tag.values)\n","\n","tokenizer = Tokenizer(lower=True,oov_token=\"UNK\")\n","tokenizer.fit_on_texts(df_train['Sentence'])\n","\n","train_inputs = tokenizer.texts_to_sequences(df_train['Sentence'])\n","test_inputs = tokenizer.texts_to_sequences(df_test['Sentence'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["word2idx = tokenizer.word_index\n","V = len(word2idx) # Vocab size\n","print(\"Found %s unique tokens \"%V)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_tags = set([val for sublist in train_targets for val in sublist])\n","test_tags = set([val for sublist in test_targets for val in sublist])\n","\n","print(\"Unique NER tags in train set: \",train_tags)\n","print(\"Unique NER tags in test set: \",test_tags)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tag_tokenizer = Tokenizer()\n","tag_tokenizer.fit_on_texts(train_targets)\n","train_tgt_int = tag_tokenizer.texts_to_sequences(train_targets)\n","test_tgt_int = tag_tokenizer.texts_to_sequences(test_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Max length\n","max_length_train = max(len(sent) for sent in train_inputs)\n","max_length_test = max(len(sent) for sent in test_inputs)\n","max_length = max(max_length_train,max_length_test)\n","\n","# Pad input sequences\n","train_inputs_final = pad_sequences(train_inputs, maxlen=max_length, padding=\"post\")\n","print(\"Shape of train inputs: \",train_inputs_final.shape)\n","\n","test_inputs_final = pad_sequences(test_inputs, maxlen=max_length, padding=\"post\")\n","print(\"Shape of test inputs: \",test_inputs_final.shape)\n","\n","train_targets_final = pad_sequences(train_tgt_int, maxlen=max_length, padding=\"post\")\n","print(\"Shape of train targets: \",train_targets_final.shape)\n","\n","test_targets_final = pad_sequences(test_tgt_int, maxlen=max_length, padding=\"post\")\n","print(\"Shape of test targets: \",test_targets_final.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Number of classes\n","\n","K = len(tag_tokenizer.word_index)  +1\n","K"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Embedding, Dropout, LSTM, TimeDistributed, Dense, Bidirectional\n","from tensorflow.keras.models import Model\n","\n","# Create a MirroredStrategy for multi-GPU support\n","strategy = tf.distribute.MirroredStrategy()\n","\n","# Define the model inside the strategy scope\n","with strategy.scope():\n","    vector_size = 128\n","\n","    i = Input(shape=(max_length,))\n","    x = Embedding(input_dim=V+1, output_dim=vector_size, mask_zero=True)(i)\n","    x = Dropout(0.2)(x)\n","    x = Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.2))(x)\n","    x = Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2))(x)\n","    x = TimeDistributed(Dense(K, activation='softmax'))(x)\n","\n","    model = Model(i, x)\n","    model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","# Set the visible GPUs\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Change this to the GPU IDs you want to use\n","\n","# Limit GPU memory growth\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)\n","\n","# Create data pipelines\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs_final, train_targets_final))\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs_final, test_targets_final))\n","\n","# Define callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n","lr_scheduler = LearningRateScheduler(lambda epoch: 0.001 * 0.9 ** epoch)\n","\n","# Compile the model inside the strategy scope\n","with strategy.scope():\n","    model.compile(optimizer=\"adam\",\n","                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                  metrics=[\"accuracy\"])\n","\n","# Fit the model\n","model.fit(train_dataset.batch(32),  # Adjust the batch size based on your GPU memory\n","          epochs=5,\n","          validation_data=test_dataset.batch(32),\n","          callbacks=[early_stopping, lr_scheduler])\n","\n","# Save the model\n","model.save('ner_model.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pickle\n","\n","# saving\n","with open('tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sentence = \"Polish Prime Minister Jaroslaw Kaczynski has voiced support for the deployment of 10 U.S. missile interceptors in Poland and guidance technology in the Czech Republic .\"\n","sentence = preprocessing_stage(sentence)\n","predictions = model.predict(pad_sequences(tokenizer.texts_to_sequences([sentence]),\n","                                          maxlen=max_length,\n","                                         padding=\"post\"))\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prediction_ner = np.argmax(predictions,axis=-1)\n","prediction_ner"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NER_tags = [tag_tokenizer.index_word[num] for num in list(prediction_ner.flatten())]\n","NER_tags[:len(tokenizer.texts_to_sequences([sentence])[0])], sentence"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1861688,"sourceId":3043695,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
